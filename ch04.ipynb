{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f40e515c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc099cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7696d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_column=None, transform=None, is_train=True): ## 기본값 필수\n",
    "        self.dataframe = dataframe.copy() ## 복사본\n",
    "        self.target_column = target_column\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self._preprocess()\n",
    "        \n",
    "        # ❗훈련세트와 테스트세트는 다르기 때문에\n",
    "        if self.is_train and target_column:\n",
    "            self.targets = self.dataframe[target_column].values\n",
    "            self.features = self.dataframe.drop([target_column], axis=1).values\n",
    "        else:\n",
    "            self.targets = None\n",
    "            self.features = self.dataframe.values\n",
    "    \n",
    "    # _: 내부에서만 쓰고 외부에선 부르지마라\n",
    "    def _preprocess(self):\n",
    "        # 불필요한 컬럼을 삭제\n",
    "        columns_to_drop = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n",
    "        existing_columns = [\n",
    "            col for col in columns_to_drop if col in self.dataframe.columns\n",
    "        ]\n",
    "        if existing_columns:\n",
    "            self.dataframe.drop(existing_columns, axis=1, inplace=True)\n",
    "\n",
    "        # 나이 결측값 처리 (중앙값)\n",
    "        if \"Age\" in self.dataframe.columns:\n",
    "            self.dataframe[\"Age\"].fillna(self.dataframe[\"Age\"].median(), inplace=True)\n",
    "\n",
    "        # 승선항구 결측값 처리 (최빈값: 제일많은 빈도)\n",
    "        if \"Embarked\" in self.dataframe.columns:\n",
    "            self.dataframe[\"Embarked\"].fillna(\n",
    "                self.dataframe[\"Embarked\"].mode()[0], inplace=True\n",
    "            )\n",
    "\n",
    "        # 요금 (중앙값)\n",
    "        if \"Fare\" in self.dataframe.columns:\n",
    "            self.dataframe[\"Fare\"].fillna(self.dataframe[\"Fare\"].median(), inplace=True)\n",
    "\n",
    "        # 새로운 특성\n",
    "        if \"SibSp\" in self.dataframe.columns and \"Parch\" in self.dataframe.columns:\n",
    "            self.dataframe[\"FamilySize\"] = (\n",
    "                self.dataframe[\"SibSp\"] + self.dataframe[\"Parch\"] + 1\n",
    "            )\n",
    "            self.dataframe[\"IsAlone\"] = (self.dataframe[\"FamilySize\"] == 1).astype(int)\n",
    "        \n",
    "        # 나이 그룹\n",
    "        if \"Age\" in self.dataframe.columns:\n",
    "            self.dataframe[\"AgeGroup\"] = pd.cut(\n",
    "                self.dataframe[\"Age\"],\n",
    "                bins=[0, 12, 18, 35, 60, 100],\n",
    "                labels=[0, 1, 2, 3, 4],\n",
    "            ).astype(int)\n",
    "\n",
    "        # 요금 그룹\n",
    "        if \"Fare\" in self.dataframe.columns:\n",
    "            self.dataframe[\"FareGroup\"] = pd.qcut(\n",
    "                self.dataframe[\"Fare\"], q=4, labels=[0, 1, 2, 3]\n",
    "            ).astype(int)\n",
    "\n",
    "        # 원-핫 인코딩\n",
    "        if \"Sex\" in self.dataframe.columns:\n",
    "            sex_dummies = pd.get_dummies(self.dataframe[\"Sex\"], drop_first=True)\n",
    "            self.dataframe = pd.concat([self.dataframe, sex_dummies], axis=1)\n",
    "            self.dataframe.drop([\"Sex\"], axis=1, inplace=True)\n",
    "\n",
    "        if \"Embarked\" in self.dataframe.columns:\n",
    "            embarked_dummies = pd.get_dummies(\n",
    "                self.dataframe[\"Embarked\"], drop_first=True\n",
    "            )\n",
    "            self.dataframe = pd.concat([self.dataframe, embarked_dummies], axis=1)\n",
    "            self.dataframe.drop([\"Embarked\"], axis=1, inplace=True)\n",
    "\n",
    "        # 나머지 결측 (평균)\n",
    "        self.dataframe.fillna(self.dataframe.mean(), inplace=True)\n",
    "        print(f\"전처리 후 특성 수: {len(self.dataframe.columns)}\")\n",
    "        print(f\"특성 목록: {list(self.dataframe.columns)}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        features = self.features[idx]\n",
    "\n",
    "        # 변환 적용\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        features = torch.FloatTensor(features)\n",
    "\n",
    "        if self.is_train and self.targets is not None:\n",
    "            target = torch.LongTensor([self.targets[idx]])[0]\n",
    "            return features, target\n",
    "        else:\n",
    "            return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7adb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2. 데이터 변환 클래스\n",
    "class StandardScaleTransform:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "\n",
    "    # torch가 아니라 사이키런을 사용하는거기때문에\n",
    "    def fit(self, data):\n",
    "        self.scaler.fit(data)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\n",
    "                \"스케일러가 아직 학습되지 않았습니다. fit() 메서드를 먼저 호출하세요.\"\n",
    "            )\n",
    "\n",
    "        if sample.ndim == 1:\n",
    "            sample = sample.reshape(1, -1)\n",
    "            return self.scaler.transform(sample).flatten()\n",
    "        else:\n",
    "            return self.scaler.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc717f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후 특성 수: 13\n",
      "특성 목록: ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone', 'AgeGroup', 'FareGroup', 'male', 'Q', 'S']\n",
      "전처리 후 특성 수: 12\n",
      "특성 목록: ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone', 'AgeGroup', 'FareGroup', 'male', 'Q', 'S']\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 train, test로 나뉘어진 경우 속성이 다르다 == test.csv엔 target값이 없다\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_data =TitanicDataset(df_train, target_column=\"Survived\")\n",
    "test_data =TitanicDataset(df_test, is_train=False) \n",
    "\n",
    "transform = StandardScaleTransform()\n",
    "transform.fit(train_data.features)\n",
    "\n",
    "train_data.transform = transform\n",
    "test_data.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d288cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.features, train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48305694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(train_data, [0.2, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dccaf503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b8cb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a316c",
   "metadata": {},
   "source": [
    "# 모델(은닉층 구성)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96da4e",
   "metadata": {},
   "source": [
    "### 활성화 함수\n",
    "- 은닉층\n",
    "    - ❗ReLU:: 모르겠으면 렐루\n",
    "    - ❗Leaky ReLU\n",
    "    - ELU\n",
    "    - (LSTM)\n",
    "    - ❗Tanh\n",
    "    - SiLU:: 최근 연구에서 가장 좋다고 알려짐\n",
    "\n",
    "- 출력층\n",
    "    - ❗Sigmoid (이진분류)\n",
    "    - ❗Softmax (다진분류)\n",
    "    - ❗Linear (회귀)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6980a",
   "metadata": {},
   "source": [
    "- 타이타닉 데이터:: ```Input --> Hidden --> Output```\n",
    "    - Input으로 12개(특성 개수)가 들어감\n",
    "    - Output으로 2가지(Survived)가 나옴 ==> ``시그모이드 함수``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "017b8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = train_data.features.shape[1]\n",
    "# input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7dd334",
   "metadata": {},
   "source": [
    "- nn.Linear(input_size, 256) \n",
    "    - 12개를 넣어서 hidden_size가 256개\n",
    "    - hidden_size든 몇개든 상관xx __ 클수록 좋겠지..\n",
    "- nn.BatchNorm1d(256)\n",
    "    - 배치 정규화\n",
    "- nn.ReLU() \n",
    "    - 활성화 함수\n",
    "- nn.Dropout(0.5) \n",
    "    - Dropout Layer:: 50%확률로 아무거나 자름 -> 떨어지는게 정상이지만 ''성능이 올라감'', 내려갈때도 있음\\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7252fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=[256, 128, 64], dropout_rate=0.3):\n",
    "        super(TitanicNet, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        for i, hidden_size in enumerate(hidden_size):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size), \n",
    "                nn.BatchNorm1d(hidden_size), \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate), \n",
    "            ])       \n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # 출력층\n",
    "        layers.append(nn.Linear(prev_size, 2))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):   # 순전파\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4d4d2",
   "metadata": {},
   "source": [
    "# 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7647fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    num_epochs=100,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"모델 훈련 함수\"\"\"\n",
    "    # 상태값\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    patience = 20  # 조기 종료를 위한 patience\n",
    "    \n",
    "    # 학습데이터\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # 학습모델\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "       \n",
    "        for batch_idx, (data, target) in enumerate(train_loader): # 배치사이즈만큼 돌때는 DataLoader(빈부분알아서 처리)가 필요함\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            print(output.shape)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward() # 자동미분\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_train += target.size(0)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "\n",
    "        # 검증 모드\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total_val += target.size(0)\n",
    "                correct_val += (predicted == target).sum().item()\n",
    "\n",
    "        # 평균 계산\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100.0 * correct_train / total_train\n",
    "        val_acc = 100.0 * correct_val / total_val\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # 학습률 스케줄러\n",
    "        if scheduler:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # 최고 성능 모델 저장\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(\n",
    "                    f\"=*=*=*= Validation Loss decreased to {avg_val_loss:.6f}. Saving the model! =*=*=*=\"\n",
    "                )\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "                f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "        # 조기 종료\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6f9fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. 모델 훈련 시작...\n",
      "사용 장치: cpu\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "Epoch [20/300] - Train Loss: 0.3485, Train Acc: 86.03%, Val Loss: 0.5156, Val Acc: 80.62%\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([19, 2])\n",
      "Early stopping at epoch 29\n"
     ]
    }
   ],
   "source": [
    "input_size = train_data.features.shape[1]\n",
    "model = TitanicNet(input_size)\n",
    "criterion = nn.CrossEntropyLoss()   # 손실함수와 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=10, factor=0.5\n",
    ")\n",
    "\n",
    "# 5. 모델 훈련\n",
    "print(f\"\\n5. 모델 훈련 시작...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 장치: {device}\")\n",
    "\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=300,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bda230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TitanicNet(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "    (12): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "# 결과:: 3층짜리 은닉층 + (마지막층(12)) 출 력층"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
